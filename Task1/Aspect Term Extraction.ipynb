{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12271279,"sourceType":"datasetVersion","datasetId":6819616}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install seqeval\n# SeqEval is a Python library used for evaluating sequence labeling tasks,\n# particularly in natural language processing (NLP).\n# SeqEval computes common evaluation metrics like precision,\n# recall, and F1-score for these tasks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:05.018136Z","iopub.execute_input":"2025-06-24T16:30:05.018487Z","iopub.status.idle":"2025-06-24T16:30:08.414950Z","shell.execute_reply.started":"2025-06-24T16:30:05.018466Z","shell.execute_reply":"2025-06-24T16:30:08.414087Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import sys\nsys.path.insert(0, '/kaggle/input/aspect-based-sent-analysis')\n\n# sys is a module\n# sys.path: This is a list in Python that contains directories where \n# Python looks for modules to import.\n\n# This line adds the directory /kaggle/input/conlleval \n# to the beginning (index 0) of the sys.path list.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.416076Z","iopub.execute_input":"2025-06-24T16:30:08.416300Z","iopub.status.idle":"2025-06-24T16:30:08.420206Z","shell.execute_reply.started":"2025-06-24T16:30:08.416281Z","shell.execute_reply":"2025-06-24T16:30:08.419449Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# These are standard library module \nimport io \n# Provides tools for handling I/O (input and output), such as working with streams, \n# file-like objects, or buffer handling.\nimport os\n# Navigating directories or retrieving environment variables\nimport json\n# SON (JavaScript Object Notation) data for serialization (converting objects to JSON) \n# and deserialization (reading JSON into objects).\nimport sys\n# discussed above\n\n# These are third party library module (installed Separately)\nimport numpy as np\n# powerful library for numerical computing, especially for arrays and matrices\n# Matrix operations, mathematical computations, or scientific tasks.\nfrom seqeval.metrics import f1_score\n# metrics in seqeval.metrics is essentially a class and f1_score is a function\n\nimport torch\n#  Part of PyTorch, a popular framework for deep learning, \n#  enabling the creation and training of neural networks.\nimport torch.nn as nn\n# Provides tools to build neural network layers.\nimport torch.optim as optim\n# Implements optimization algorithms like SGD and Adam.\nfrom torch.utils.data import Dataset, DataLoader\n# Dataset: An abstract class in PyTorch that represents a dataset\n# A utility that wraps a dataset and enables easier \n# batch loading, shuffling, and multiprocessing.\n\nimport matplotlib.pyplot as plt\n# A plotting library for creating static, interactive, or animated visualizations.\nfrom tqdm import tqdm\n# Displays progress bars in loops and tasks to track execution.\n# The full form of tqdm is \"taqaddum,\" which means \"progress\" in Arabic\nfrom conlleval import evaluate\n# utilizing the evaluate function from conlleval class\nimport sys, io\nimport json\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.421622Z","iopub.execute_input":"2025-06-24T16:30:08.421859Z","iopub.status.idle":"2025-06-24T16:30:08.436325Z","shell.execute_reply.started":"2025-06-24T16:30:08.421840Z","shell.execute_reply":"2025-06-24T16:30:08.435587Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def preprocess_data(input_path, output_path):\n    \n\n    def create_labels(merged_tokens, merged_offsets, aspect_terms):\n\n        labels = [\"O\"] * len(merged_tokens)\n        for aspect in aspect_terms:\n            a_from = int(aspect[\"from\"])\n\n            a_to = int(aspect[\"to\"])\n            token_found = False\n            for i, (token, (start, end)) in enumerate(zip(merged_tokens, merged_offsets)):\n                effective_end = end - 1 if token.endswith((\".\", \",\")) else end\n\n                if start >= a_from and effective_end <= a_to:\n                    labels[i] = \"B\" if not token_found else \"I\"\n                    token_found = True\n                # token_found is used to handle multi-term aspect \n                # a_from and effective_end indicates the broad level of aspect term\n                # whle start and end indicate words inside multi-term aspect term\n\n        return labels\n    \n\n    def merge_tokens_with_punctuation(tokens, token_offsets):\n        merged_tokens = []\n        merged_offsets = []\n\n        for i, (token, (start, end)) in enumerate(zip(tokens, token_offsets)):\n            # zip(tokens, token_offsets):Combines the tokens list and \n            # the token_offsets list into pairs (tuples).\n            if token in [\".\", \",\"]:\n                if merged_tokens:\n                    prev_token = merged_tokens[-1]\n                    prev_start, _ = merged_offsets[-1]\n\n                    merged_tokens[-1] = prev_token + token\n\n                    merged_offsets[-1] = (prev_start, end)\n                else:\n                    merged_tokens.append(token)\n\n                    merged_offsets.append((start, end))\n            else:\n                merged_tokens.append(token)\n                merged_offsets.append((start, end))\n\n        return merged_tokens, merged_offsets\n    \n\n    \n    \n    def get_token_offsets(sentence, tokens):\n\n        token_offsets = []\n        start_index = 0\n\n        for token in tokens:\n            index = sentence.find(token, start_index)\n            # The method returns the index of the first occurrence of the substring.\n            # If the substring is not found, it returns -1\n            if index == -1:\n\n                index = start_index\n\n            token_offsets.append((index, index + len(token)))\n            start_index = index + len(token)\n\n        return token_offsets\n\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n# Input Type: File-like object containing JSON data.\n# Output Type: Python object (dict, list, etc.), depending on the JSON structure.\n# if file contains list of json objects then output will bw list of dictionaries.\n    \n    preprocessed = []\n\n    for entry in data:\n\n        sentence = entry[\"sentence\"]\n\n        tokens = sentence.split()\n        \n        token_offsets = get_token_offsets(sentence, tokens)\n\n        merged_tokens, merged_offsets = merge_tokens_with_punctuation(tokens, token_offsets)\n\n        labels = create_labels(merged_tokens, merged_offsets, entry.get(\"aspect_terms\", []))\n        \n        preprocessed.append({\n            \"sentence\": sentence,\n            \"tokens\": merged_tokens,\n            \"labels\": labels,\n            \"aspect_terms\": [a[\"term\"] for a in entry.get(\"aspect_terms\", [])]\n        })\n        \n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(preprocessed, f, indent=2)\n    print(f\"Preprocessed data saved to {output_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.437702Z","iopub.execute_input":"2025-06-24T16:30:08.437994Z","iopub.status.idle":"2025-06-24T16:30:08.468155Z","shell.execute_reply.started":"2025-06-24T16:30:08.437968Z","shell.execute_reply":"2025-06-24T16:30:08.467529Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def build_vocab(dataset, min_freq=1):\n\n    \n    def create_vocab_mapping(word_freq):\n\n        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n        for word, freq in word_freq.items():\n            if freq >= min_freq:\n\n                vocab[word] = len(vocab)\n        return vocab\n    \n    def count_word_frequencies():\n\n        word_freq = {}\n\n        for entry in dataset:\n            for token in entry[\"tokens\"]:\n\n                word_freq[token] = word_freq.get(token, 0) + 1\n        return word_freq\n    \n\n    word_freq = count_word_frequencies()\n    return create_vocab_mapping(word_freq)\n\n# Label mappings\nlabel2id = {\"O\": 0, \"B\": 1, \"I\": 2}\nid2label = {0: \"O\", 1: \"B\", 2: \"I\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.469176Z","iopub.execute_input":"2025-06-24T16:30:08.469496Z","iopub.status.idle":"2025-06-24T16:30:08.493378Z","shell.execute_reply.started":"2025-06-24T16:30:08.469467Z","shell.execute_reply":"2025-06-24T16:30:08.492652Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class AspectDataset(Dataset):\n    def __init__(self, data, vocab, label2id, max_len=100):\n        self.data = data\n        self.vocab = vocab\n        self.label2id = label2id\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        def get_raw_tokens_and_labels():\n            entry = self.data[idx]\n\n\n            tokens = entry[\"tokens\"]\n            labels = entry[\"labels\"]\n            token_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n\n            label_ids = [self.label2id[label] for label in labels]\n\n            return tokens, token_ids, label_ids\n\n        def pad_and_truncate(tokens, token_ids, label_ids):\n\n            orig_len = len(token_ids)\n\n            if orig_len < self.max_len:\n                pad_length = self.max_len - orig_len\n                token_ids.extend([self.vocab[\"<PAD>\"]] * pad_length)\n\n                label_ids.extend([-100] * pad_length)\n            else:\n                token_ids = token_ids[:self.max_len]\n\n                label_ids = label_ids[:self.max_len]\n\n                tokens = tokens[:self.max_len]\n                orig_len = self.max_len\n\n            return tokens, token_ids, label_ids, orig_len\n\n        tokens, token_ids, label_ids = get_raw_tokens_and_labels()\n\n        tokens, token_ids, label_ids, orig_len = pad_and_truncate(tokens, token_ids, label_ids)\n\n        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label_ids, dtype=torch.long), orig_len, tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.494228Z","iopub.execute_input":"2025-06-24T16:30:08.494441Z","iopub.status.idle":"2025-06-24T16:30:08.517301Z","shell.execute_reply.started":"2025-06-24T16:30:08.494423Z","shell.execute_reply":"2025-06-24T16:30:08.516717Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def custom_collate(batch):\n    \n    \n    token_ids = torch.stack([b[0] for b in batch])\n\n    label_ids = torch.stack([b[1] for b in batch])\n\n    orig_lens = [b[2] for b in batch]\n    tokens = [b[3] for b in batch]  \n\n    return token_ids, label_ids, orig_lens, tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.518092Z","iopub.execute_input":"2025-06-24T16:30:08.518288Z","iopub.status.idle":"2025-06-24T16:30:08.545219Z","shell.execute_reply.started":"2025-06-24T16:30:08.518272Z","shell.execute_reply":"2025-06-24T16:30:08.544370Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class SequenceLabelingModel(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, \n                 rnn_type=\"RNN\", embedding_matrix=None, num_layers=1, dropout=0.5):\n        \n        super(SequenceLabelingModel, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        if embedding_matrix is not None:\n            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float))\n        \n        # For single layer, PyTorch's dropout parameter is ignored, so we set it to 0 in that case.\n        rnn_dropout = dropout if num_layers > 1 else 0\n        \n        if rnn_type.upper() == \"RNN\":\n            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, \n                              batch_first=True, bidirectional=True, dropout=rnn_dropout)\n        elif rnn_type.upper() == \"GRU\":\n            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, \n                              batch_first=True, bidirectional=True, dropout=rnn_dropout)\n        else:\n            raise ValueError(\"Unsupported rnn_type: choose either 'RNN' or 'GRU'\")\n        \n        \n        self.dropout = nn.Dropout(dropout)\n\n        self.fc = nn.Linear(hidden_dim * 2, num_labels)\n\n    def forward(self, x):\n        embeds = self.embedding(x)          \n\n        outputs, _ = self.rnn(embeds)  \n\n        outputs = self.dropout(outputs)         \n        logits = self.fc(outputs)               \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.547025Z","iopub.execute_input":"2025-06-24T16:30:08.547241Z","iopub.status.idle":"2025-06-24T16:30:08.565350Z","shell.execute_reply.started":"2025-06-24T16:30:08.547223Z","shell.execute_reply":"2025-06-24T16:30:08.564701Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def load_embeddings(embedding_path, vocab, embedding_dim):\n\n    def initialize_embeddings():\n        return np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim)).astype(np.float32)\n    \n    \n    def update_embeddings(embeddings):\n\n        with open(embedding_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                try:\n                    values = line.rstrip().split()\n                    if len(values) < embedding_dim + 1:\n                        continue\n                    word = values[0]\n\n                    vector = np.asarray(values[1:], dtype=\"float32\")\n                    if word in vocab:\n                        embeddings[vocab[word]] = vector\n                except (ValueError, IndexError):\n                    continue\n        return embeddings\n\n    embeddings = initialize_embeddings()\n\n    return update_embeddings(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.566474Z","iopub.execute_input":"2025-06-24T16:30:08.566778Z","iopub.status.idle":"2025-06-24T16:30:08.586868Z","shell.execute_reply.started":"2025-06-24T16:30:08.566751Z","shell.execute_reply":"2025-06-24T16:30:08.586259Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def compute_predictions(model, data_loader, criterion, device):\n\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n\n    all_labels = []\n    \n    with torch.no_grad():\n        for token_ids, label_ids, seq_len, tokens_batch in data_loader:\n            token_ids = token_ids.to(device)\n\n            label_ids = label_ids.to(device)\n            logits = model(token_ids) \n            loss = criterion(logits.view(-1, logits.shape[-1]), label_ids.view(-1))\n\n            total_loss += loss.item()\n            predictions = torch.argmax(logits, dim=-1)  \n            predictions = predictions.cpu().numpy()\n            label_ids = label_ids.cpu().numpy()\n            batch_size = token_ids.size(0)\n            for i in range(batch_size):\n\n                length = seq_len[i]\n                pred_tags = [id2label[p] if id2label[p] == \"O\" else id2label[p] + \"-TERM\" \n                             \n                           for p in predictions[i][:length]]\n                true_tags = [id2label[label_ids[i][j]] if id2label[label_ids[i][j]] == \"O\" \n                           else id2label[label_ids[i][j]] + \"-TERM\" for j in range(length)]\n                all_preds.append(pred_tags)\n\n                all_labels.append(true_tags)\n\n    \n    avg_loss = total_loss / len(data_loader)\n\n    return avg_loss, all_preds, all_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.587736Z","iopub.execute_input":"2025-06-24T16:30:08.588026Z","iopub.status.idle":"2025-06-24T16:30:08.618909Z","shell.execute_reply.started":"2025-06-24T16:30:08.587997Z","shell.execute_reply":"2025-06-24T16:30:08.618255Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def calculate_metrics(all_labels, all_preds):\n\n    old_stdout = sys.stdout\n    sys.stdout = io.StringIO()\n    \n    flat_true = [tag for sentence in all_labels for tag in sentence]\n\n    flat_pred = [tag for sentence in all_preds for tag in sentence]\n    \n    precision, recall, f1 = evaluate(flat_true, flat_pred)\n    sys.stdout = old_stdout\n    \n    from seqeval.metrics import f1_score\n\n    tag_f1 = f1_score(all_labels, all_preds)\n    \n    return precision, recall, f1/100, tag_f1\n\ndef evaluate_model(model, data_loader, criterion, device):\n\n    avg_loss, all_preds, all_labels = compute_predictions(model, data_loader, criterion, device)\n    precision, recall, f1, tag_f1 = calculate_metrics(all_labels, all_preds)\n    \n    print(f\"\\n\\n\\nChunk-level metrics:\")\n\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n\n    print(f\"F1: {f1:.2f}\")\n\n    print(f\"\\nTag-level F1: {tag_f1:.2f}\\n\\n\\n\")\n    \n    return avg_loss, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.619693Z","iopub.execute_input":"2025-06-24T16:30:08.619896Z","iopub.status.idle":"2025-06-24T16:30:08.645804Z","shell.execute_reply.started":"2025-06-24T16:30:08.619880Z","shell.execute_reply":"2025-06-24T16:30:08.645194Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs, lr, device, model_save_path):\n\n    def setup_training():\n        model.to(device)\n\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n\n        criterion = nn.CrossEntropyLoss(ignore_index=-100)\n\n        return optimizer, criterion, [], [], 0.0\n\n    def train_epoch(optimizer, criterion):\n\n        model.train()\n\n        epoch_loss = 0.0\n        for token_ids, label_ids, _, _ in train_loader:\n\n            token_ids = token_ids.to(device)\n\n            label_ids = label_ids.to(device)\n\n            optimizer.zero_grad()\n\n            logits = model(token_ids)\n\n            loss = criterion(logits.view(-1, logits.shape[-1]), label_ids.view(-1))\n\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n        return epoch_loss / len(train_loader)\n\n    def evaluate_and_save(val_f1, best_val_f1, no_improve_count):\n\n        if val_f1 > best_val_f1:\n\n            best_val_f1 = val_f1\n\n            torch.save(model.state_dict(), model_save_path)\n            no_improve_count = 0\n        else:\n            no_improve_count += 1\n\n        return best_val_f1, no_improve_count\n\n    optimizer, criterion, train_losses, val_losses, best_val_f1 = setup_training()\n\n    patience = 10\n    no_improve_count = 0\n\n    for epoch in range(num_epochs):\n\n        avg_train_loss = train_epoch(optimizer, criterion)\n\n        train_losses.append(avg_train_loss)\n        \n        val_loss, val_f1 = evaluate_model(model, val_loader, criterion, device)\n        val_losses.append(val_loss)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n        \n        best_val_f1, no_improve_count = evaluate_and_save(val_f1, best_val_f1, no_improve_count)\n        \n        if no_improve_count >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    return train_losses, val_losses, best_val_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.646530Z","iopub.execute_input":"2025-06-24T16:30:08.646734Z","iopub.status.idle":"2025-06-24T16:30:08.671888Z","shell.execute_reply.started":"2025-06-24T16:30:08.646713Z","shell.execute_reply":"2025-06-24T16:30:08.671066Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def setup_inference_model(vocab, model_embedding, model_rnn, device):\n\n    # Determine embedding file and dimension\n    if model_embedding.lower() == \"glove\":\n        embedding_dim = 300\n        embedding_file = \"/kaggle/input/aspect-based-sent-analysis/glove.6B.300d.txt\"\n\n    elif model_embedding.lower() == \"fasttext\":\n        embedding_dim = 300\n        embedding_file = \"/kaggle/input/aspect-based-sent-analysis/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\"\n\n    else:\n        raise ValueError(\"Unsupported embedding type for inference\")\n        \n    embedding_matrix = load_embeddings(embedding_file, vocab, embedding_dim)\n\n    model = SequenceLabelingModel(\n            vocab_size=len(vocab),\n            embedding_dim=embedding_dim,\n            hidden_dim=128,\n            num_labels=len(label2id),\n            rnn_type=model_rnn,\n            embedding_matrix=embedding_matrix,\n            num_layers=2,\n            dropout=0.5\n        )\n\n    model_save_path = \"/kaggle/input/aspect-based-sent-analysis/best_model_GRU_fasttext.pth\"\n\n    if not os.path.exists(model_save_path):\n        raise FileNotFoundError(f\"Saved model file {model_save_path} not found.\")\n    \n    model.load_state_dict(torch.load(model_save_path, map_location=device))\n    model.to(device)\n\n    return model\n\ndef run_inference(args, vocab, device):\n    \n    # Prepare test data\n    if not os.path.exists(\"test.task1.json\"):\n        preprocess_data(args.test, \"test.task1.json\")\n\n    with open(\"test.task1.json\", \"r\", encoding=\"utf-8\") as f:\n        test_data = json.load(f)\n    test_dataset = AspectDataset(test_data, vocab, label2id, max_len=args.max_len)\n\n    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=custom_collate)\n\n\n    try:\n        model = setup_inference_model(vocab, args.model_embedding, args.model_rnn, device)\n\n        criterion = nn.CrossEntropyLoss(ignore_index=-100)\n        test_loss, test_f1 = evaluate_model(model, test_loader, criterion, device)\n\n        print(f\"Test Loss: {test_loss:.4f}, Test F1: {test_f1:.4f}\")\n    except (ValueError, FileNotFoundError) as e:\n        print(f\"Error during inference: {str(e)}\")\n\n\n    # with open(\"test_predictions.txt\", \"w\") as f:\n    #  for tokens, preds in zip(tokens_batch, all_preds):\n    #     for token, tag in zip(tokens, preds):\n    #         f.write(f\"{token}\\t{tag}\\n\")\n    #     f.write(\"\\n\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.672633Z","iopub.execute_input":"2025-06-24T16:30:08.672853Z","iopub.status.idle":"2025-06-24T16:30:08.699912Z","shell.execute_reply.started":"2025-06-24T16:30:08.672836Z","shell.execute_reply":"2025-06-24T16:30:08.699113Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def main():\n   \n    args = lambda: None  \n    # This method is a quick and simple way to create an empty object \n    # that can have arbitrary attributes assigned to it.\n    \n    args.train = \"/kaggle/input/aspect-based-sent-analysis/train.json\"\n    args.val = \"/kaggle/input/aspect-based-sent-analysis/val.json\"\n    args.test = \"/kaggle/input/aspect-based-sent-analysis/test_task1.json\"\n    args.epochs = 100\n    args.lr = 0.0005\n    args.max_len = 100\n    args.batch_size = 32\n    args.inference = False  \n    args.model_rnn = \"GRU\"\n    args.model_embedding = \"fasttext\"\n    \n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n   \n    if not os.path.exists(\"train.task1.json\"):\n      preprocess_data(args.train, \"train.task1.json\")\n#   if preprocessing not done then perform preprocessing  \n\n    with open(\"train.task1.json\", \"r\", encoding=\"utf-8\") as f:\n        train_data = json.load(f)\n\n    vocab = build_vocab(train_data)\n\n    \n    with open(\"/kaggle/working/vocab_task1.json\",\"w\") as f:\n     json.dump(vocab, f)\n\n    print(\"Vocabulary size:\", len(vocab))\n\n    # If inference flag is set, run inference and exit\n    if args.inference:\n        run_inference(args, vocab, device)\n        return\n\n    \n    if not os.path.exists(\"val.task1.json\"):\n        preprocess_data(args.val, \"val.task1.json\")\n\n    with open(\"val.task1.json\", \"r\", encoding=\"utf-8\") as f:\n        val_data = json.load(f)\n\n\n    model_configs = [\n        {\"rnn_type\": \"RNN\", \"embedding\": \"glove\", \"embedding_path\": \"/kaggle/input/aspect-based-sent-analysis/glove.6B.300d.txt\", \"embedding_dim\": 300},\n        {\"rnn_type\": \"RNN\", \"embedding\": \"fasttext\", \"embedding_path\": \"/kaggle/input/aspect-based-sent-analysis/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\", \"embedding_dim\": 300},\n        {\"rnn_type\": \"GRU\", \"embedding\": \"glove\", \"embedding_path\": \"/kaggle/input/aspect-based-sent-analysis/glove.6B.300d.txt\", \"embedding_dim\": 300},\n        {\"rnn_type\": \"GRU\", \"embedding\": \"fasttext\", \"embedding_path\": \"/kaggle/input/aspect-based-sent-analysis/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\", \"embedding_dim\": 300},\n    ]\n\n    \n    train_dataset = AspectDataset(train_data, vocab, label2id, max_len=args.max_len)\n    val_dataset = AspectDataset(val_data, vocab, label2id, max_len=args.max_len)\n    \n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=custom_collate)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=custom_collate)\n\n\n    \n   \n\n    results = {}\n    for config in model_configs:\n        print(\"\\n========================================\")\n        print(f\"Training Model: {config['rnn_type']} with {config['embedding']}\")\n        print(\"========================================\")\n\n        embedding_matrix = load_embeddings(config[\"embedding_path\"], vocab, config[\"embedding_dim\"])\n\n        model = SequenceLabelingModel(\n                vocab_size=len(vocab),\n                embedding_dim=config[\"embedding_dim\"],\n                hidden_dim=128,\n                num_labels=len(label2id),\n                rnn_type=config[\"rnn_type\"],\n                embedding_matrix=embedding_matrix,\n                num_layers=2,\n                dropout=0.5\n            )\n\n        model_save_path = f\"best_model_{config['rnn_type']}_{config['embedding']}.pth\"\n\n        train_losses, val_losses, best_f1 = train_model(\n            model, train_loader, val_loader,\n            num_epochs=args.epochs, lr=args.lr, device=device,\n            model_save_path=model_save_path\n        )\n        # Plot loss curves\n        plt.figure()\n        plt.plot(train_losses, label=\"Train Loss\")\n        plt.plot(val_losses, label=\"Val Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plot_filename = f\"{config['rnn_type']}_{config['embedding']}_loss.png\"\n        plt.savefig(plot_filename)\n        \n        plt.close()\n        print(f\"Best Validation F1 for {config['rnn_type']} with {config['embedding']}: {best_f1:.4f}\")\n        results[f\"{config['rnn_type']}_{config['embedding']}\"] = best_f1\n\n    print(\"\\nFinal model performance on validation set:\")\n\n    for key, val in results.items():\n        print(f\"{key}: F1 = {val:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.700808Z","iopub.execute_input":"2025-06-24T16:30:08.701043Z","iopub.status.idle":"2025-06-24T16:30:08.724281Z","shell.execute_reply.started":"2025-06-24T16:30:08.701026Z","shell.execute_reply":"2025-06-24T16:30:08.723452Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\nif __name__ == \"__main__\": \n    main()\n\n# __name__: This is a built-in variable in Python. When a Python script is run, \n# this variable is automatically set to \"__main__\" for that script.\n\n# If the script is imported into another script,\n# the __name__ variable is set to the name of the module (not \"__main__\").\n\n# so The construct is used for defining a block of code that should \n# only execute when the script is run directly, and not when it is imported\n\n# That's why on import task1 Nothing prints automatically","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:30:08.725000Z","iopub.execute_input":"2025-06-24T16:30:08.725240Z","iopub.status.idle":"2025-06-24T16:33:30.608636Z","shell.execute_reply.started":"2025-06-24T16:30:08.725222Z","shell.execute_reply":"2025-06-24T16:33:30.607753Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 6360\nPreprocessed data saved to val.task1.json\n\n========================================\nTraining Model: RNN with glove\n========================================\n\n\n\nChunk-level metrics:\nPrecision: 79.00\nRecall: 47.53\nF1: 0.59\n\nTag-level F1: 0.59\n\n\n\nEpoch 1/100: Train Loss: 0.3232, Val Loss: 0.1885, Val F1: 0.5935\n\n\n\nChunk-level metrics:\nPrecision: 70.99\nRecall: 57.14\nF1: 0.63\n\nTag-level F1: 0.63\n\n\n\nEpoch 2/100: Train Loss: 0.1890, Val Loss: 0.1483, Val F1: 0.6332\n\n\n\nChunk-level metrics:\nPrecision: 76.30\nRecall: 56.59\nF1: 0.65\n\nTag-level F1: 0.65\n\n\n\nEpoch 3/100: Train Loss: 0.1369, Val Loss: 0.1489, Val F1: 0.6498\n\n\n\nChunk-level metrics:\nPrecision: 70.96\nRecall: 65.11\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 4/100: Train Loss: 0.1013, Val Loss: 0.1399, Val F1: 0.6791\n\n\n\nChunk-level metrics:\nPrecision: 75.78\nRecall: 60.16\nF1: 0.67\n\nTag-level F1: 0.67\n\n\n\nEpoch 5/100: Train Loss: 0.0698, Val Loss: 0.1562, Val F1: 0.6708\n\n\n\nChunk-level metrics:\nPrecision: 75.32\nRecall: 65.38\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 6/100: Train Loss: 0.0488, Val Loss: 0.1651, Val F1: 0.7000\n\n\n\nChunk-level metrics:\nPrecision: 65.35\nRecall: 68.41\nF1: 0.67\n\nTag-level F1: 0.67\n\n\n\nEpoch 7/100: Train Loss: 0.0324, Val Loss: 0.2080, Val F1: 0.6685\n\n\n\nChunk-level metrics:\nPrecision: 70.20\nRecall: 67.31\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 8/100: Train Loss: 0.0218, Val Loss: 0.2021, Val F1: 0.6872\n\n\n\nChunk-level metrics:\nPrecision: 68.58\nRecall: 68.96\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 9/100: Train Loss: 0.0131, Val Loss: 0.2309, Val F1: 0.6877\n\n\n\nChunk-level metrics:\nPrecision: 66.49\nRecall: 69.23\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 10/100: Train Loss: 0.0092, Val Loss: 0.2483, Val F1: 0.6783\n\n\n\nChunk-level metrics:\nPrecision: 65.94\nRecall: 66.48\nF1: 0.66\n\nTag-level F1: 0.66\n\n\n\nEpoch 11/100: Train Loss: 0.0083, Val Loss: 0.2768, Val F1: 0.6621\n\n\n\nChunk-level metrics:\nPrecision: 65.71\nRecall: 68.96\nF1: 0.67\n\nTag-level F1: 0.67\n\n\n\nEpoch 12/100: Train Loss: 0.0065, Val Loss: 0.2868, Val F1: 0.6729\n\n\n\nChunk-level metrics:\nPrecision: 70.89\nRecall: 67.58\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 13/100: Train Loss: 0.0065, Val Loss: 0.2893, Val F1: 0.6920\n\n\n\nChunk-level metrics:\nPrecision: 74.12\nRecall: 63.74\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 14/100: Train Loss: 0.0026, Val Loss: 0.3128, Val F1: 0.6854\n\n\n\nChunk-level metrics:\nPrecision: 70.59\nRecall: 65.93\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 15/100: Train Loss: 0.0025, Val Loss: 0.3136, Val F1: 0.6818\n\n\n\nChunk-level metrics:\nPrecision: 70.35\nRecall: 66.48\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 16/100: Train Loss: 0.0016, Val Loss: 0.3223, Val F1: 0.6836\nEarly stopping triggered.\nBest Validation F1 for RNN with glove: 0.7000\n\n========================================\nTraining Model: RNN with fasttext\n========================================\n\n\n\nChunk-level metrics:\nPrecision: 77.83\nRecall: 49.18\nF1: 0.60\n\nTag-level F1: 0.60\n\n\n\nEpoch 1/100: Train Loss: 0.3428, Val Loss: 0.1858, Val F1: 0.6027\n\n\n\nChunk-level metrics:\nPrecision: 73.04\nRecall: 64.01\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 2/100: Train Loss: 0.1408, Val Loss: 0.1391, Val F1: 0.6823\n\n\n\nChunk-level metrics:\nPrecision: 72.19\nRecall: 67.03\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 3/100: Train Loss: 0.0904, Val Loss: 0.1359, Val F1: 0.6952\n\n\n\nChunk-level metrics:\nPrecision: 77.85\nRecall: 61.81\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 4/100: Train Loss: 0.0529, Val Loss: 0.1590, Val F1: 0.6891\n\n\n\nChunk-level metrics:\nPrecision: 71.63\nRecall: 68.68\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 5/100: Train Loss: 0.0290, Val Loss: 0.1775, Val F1: 0.7013\n\n\n\nChunk-level metrics:\nPrecision: 73.41\nRecall: 66.76\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 6/100: Train Loss: 0.0185, Val Loss: 0.1994, Val F1: 0.6993\n\n\n\nChunk-level metrics:\nPrecision: 72.06\nRecall: 67.31\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 7/100: Train Loss: 0.0098, Val Loss: 0.2139, Val F1: 0.6960\n\n\n\nChunk-level metrics:\nPrecision: 69.48\nRecall: 65.66\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 8/100: Train Loss: 0.0054, Val Loss: 0.2378, Val F1: 0.6751\n\n\n\nChunk-level metrics:\nPrecision: 74.37\nRecall: 64.56\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 9/100: Train Loss: 0.0028, Val Loss: 0.2650, Val F1: 0.6912\n\n\n\nChunk-level metrics:\nPrecision: 67.96\nRecall: 67.58\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 10/100: Train Loss: 0.0023, Val Loss: 0.2582, Val F1: 0.6777\n\n\n\nChunk-level metrics:\nPrecision: 69.64\nRecall: 68.68\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 11/100: Train Loss: 0.0046, Val Loss: 0.2728, Val F1: 0.6916\n\n\n\nChunk-level metrics:\nPrecision: 74.60\nRecall: 64.56\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 12/100: Train Loss: 0.0033, Val Loss: 0.2962, Val F1: 0.6922\n\n\n\nChunk-level metrics:\nPrecision: 69.92\nRecall: 68.96\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 13/100: Train Loss: 0.0017, Val Loss: 0.2815, Val F1: 0.6943\n\n\n\nChunk-level metrics:\nPrecision: 71.91\nRecall: 64.01\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 14/100: Train Loss: 0.0009, Val Loss: 0.3046, Val F1: 0.6773\n\n\n\nChunk-level metrics:\nPrecision: 69.41\nRecall: 67.31\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 15/100: Train Loss: 0.0009, Val Loss: 0.2955, Val F1: 0.6834\nEarly stopping triggered.\nBest Validation F1 for RNN with fasttext: 0.7013\n\n========================================\nTraining Model: GRU with glove\n========================================\n\n\n\nChunk-level metrics:\nPrecision: 63.29\nRecall: 35.99\nF1: 0.46\n\nTag-level F1: 0.46\n\n\n\nEpoch 1/100: Train Loss: 0.3910, Val Loss: 0.2314, Val F1: 0.4588\n\n\n\nChunk-level metrics:\nPrecision: 78.24\nRecall: 56.32\nF1: 0.65\n\nTag-level F1: 0.65\n\n\n\nEpoch 2/100: Train Loss: 0.2190, Val Loss: 0.1550, Val F1: 0.6550\n\n\n\nChunk-level metrics:\nPrecision: 70.15\nRecall: 62.64\nF1: 0.66\n\nTag-level F1: 0.66\n\n\n\nEpoch 3/100: Train Loss: 0.1469, Val Loss: 0.1348, Val F1: 0.6618\n\n\n\nChunk-level metrics:\nPrecision: 70.03\nRecall: 66.76\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 4/100: Train Loss: 0.1068, Val Loss: 0.1276, Val F1: 0.6835\n\n\n\nChunk-level metrics:\nPrecision: 72.62\nRecall: 64.84\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 5/100: Train Loss: 0.0793, Val Loss: 0.1322, Val F1: 0.6851\n\n\n\nChunk-level metrics:\nPrecision: 73.31\nRecall: 68.68\nF1: 0.71\n\nTag-level F1: 0.71\n\n\n\nEpoch 6/100: Train Loss: 0.0571, Val Loss: 0.1384, Val F1: 0.7092\n\n\n\nChunk-level metrics:\nPrecision: 65.66\nRecall: 71.98\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 7/100: Train Loss: 0.0421, Val Loss: 0.1539, Val F1: 0.6868\n\n\n\nChunk-level metrics:\nPrecision: 70.59\nRecall: 69.23\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 8/100: Train Loss: 0.0300, Val Loss: 0.1604, Val F1: 0.6990\n\n\n\nChunk-level metrics:\nPrecision: 77.52\nRecall: 63.46\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 9/100: Train Loss: 0.0207, Val Loss: 0.2110, Val F1: 0.6979\n\n\n\nChunk-level metrics:\nPrecision: 66.41\nRecall: 70.60\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 10/100: Train Loss: 0.0187, Val Loss: 0.1929, Val F1: 0.6844\n\n\n\nChunk-level metrics:\nPrecision: 73.17\nRecall: 65.93\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 11/100: Train Loss: 0.0148, Val Loss: 0.2126, Val F1: 0.6936\n\n\n\nChunk-level metrics:\nPrecision: 66.84\nRecall: 70.33\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 12/100: Train Loss: 0.0091, Val Loss: 0.2161, Val F1: 0.6854\n\n\n\nChunk-level metrics:\nPrecision: 68.94\nRecall: 69.51\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 13/100: Train Loss: 0.0080, Val Loss: 0.2196, Val F1: 0.6922\n\n\n\nChunk-level metrics:\nPrecision: 71.01\nRecall: 67.31\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 14/100: Train Loss: 0.0047, Val Loss: 0.2446, Val F1: 0.6911\n\n\n\nChunk-level metrics:\nPrecision: 69.49\nRecall: 67.58\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 15/100: Train Loss: 0.0036, Val Loss: 0.2627, Val F1: 0.6852\n\n\n\nChunk-level metrics:\nPrecision: 66.41\nRecall: 70.05\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 16/100: Train Loss: 0.0041, Val Loss: 0.2611, Val F1: 0.6818\nEarly stopping triggered.\nBest Validation F1 for GRU with glove: 0.7092\n\n========================================\nTraining Model: GRU with fasttext\n========================================\n\n\n\nChunk-level metrics:\nPrecision: 63.03\nRecall: 36.54\nF1: 0.46\n\nTag-level F1: 0.46\n\n\n\nEpoch 1/100: Train Loss: 0.4254, Val Loss: 0.2528, Val F1: 0.4626\n\n\n\nChunk-level metrics:\nPrecision: 79.79\nRecall: 62.91\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 2/100: Train Loss: 0.1694, Val Loss: 0.1677, Val F1: 0.7035\n\n\n\nChunk-level metrics:\nPrecision: 76.03\nRecall: 66.21\nF1: 0.71\n\nTag-level F1: 0.71\n\n\n\nEpoch 3/100: Train Loss: 0.1030, Val Loss: 0.1516, Val F1: 0.7078\n\n\n\nChunk-level metrics:\nPrecision: 75.23\nRecall: 68.41\nF1: 0.72\n\nTag-level F1: 0.72\n\n\n\nEpoch 4/100: Train Loss: 0.0706, Val Loss: 0.1475, Val F1: 0.7165\n\n\n\nChunk-level metrics:\nPrecision: 74.76\nRecall: 63.46\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 5/100: Train Loss: 0.0486, Val Loss: 0.1699, Val F1: 0.6865\n\n\n\nChunk-level metrics:\nPrecision: 74.75\nRecall: 61.81\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 6/100: Train Loss: 0.0379, Val Loss: 0.1987, Val F1: 0.6767\n\n\n\nChunk-level metrics:\nPrecision: 73.11\nRecall: 66.48\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 7/100: Train Loss: 0.0263, Val Loss: 0.1811, Val F1: 0.6964\n\n\n\nChunk-level metrics:\nPrecision: 74.07\nRecall: 65.93\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 8/100: Train Loss: 0.0173, Val Loss: 0.2140, Val F1: 0.6977\n\n\n\nChunk-level metrics:\nPrecision: 74.06\nRecall: 65.11\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 9/100: Train Loss: 0.0125, Val Loss: 0.2328, Val F1: 0.6930\n\n\n\nChunk-level metrics:\nPrecision: 73.08\nRecall: 67.86\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 10/100: Train Loss: 0.0096, Val Loss: 0.2313, Val F1: 0.7037\n\n\n\nChunk-level metrics:\nPrecision: 71.94\nRecall: 66.21\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 11/100: Train Loss: 0.0064, Val Loss: 0.2587, Val F1: 0.6896\n\n\n\nChunk-level metrics:\nPrecision: 68.55\nRecall: 70.05\nF1: 0.69\n\nTag-level F1: 0.69\n\n\n\nEpoch 12/100: Train Loss: 0.0060, Val Loss: 0.2489, Val F1: 0.6929\n\n\n\nChunk-level metrics:\nPrecision: 67.02\nRecall: 69.78\nF1: 0.68\n\nTag-level F1: 0.68\n\n\n\nEpoch 13/100: Train Loss: 0.0049, Val Loss: 0.2612, Val F1: 0.6837\n\n\n\nChunk-level metrics:\nPrecision: 70.59\nRecall: 69.23\nF1: 0.70\n\nTag-level F1: 0.70\n\n\n\nEpoch 14/100: Train Loss: 0.0031, Val Loss: 0.2741, Val F1: 0.6990\nEarly stopping triggered.\nBest Validation F1 for GRU with fasttext: 0.7165\n\nFinal model performance on validation set:\nRNN_glove: F1 = 0.7000\nRNN_fasttext: F1 = 0.7013\nGRU_glove: F1 = 0.7092\nGRU_fasttext: F1 = 0.7165\n","output_type":"stream"}],"execution_count":43}]}