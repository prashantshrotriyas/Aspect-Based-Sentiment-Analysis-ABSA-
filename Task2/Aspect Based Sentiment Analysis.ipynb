{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12271279,"sourceType":"datasetVersion","datasetId":6819616}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport warnings\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\nfrom torch.optim import AdamW  \nfrom sklearn.metrics import accuracy_score\n\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"  # Disabling wandb\n\nwarnings.filterwarnings(\"ignore\")  # Suppress warnings\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.102557Z","iopub.execute_input":"2025-06-26T14:07:28.102844Z","iopub.status.idle":"2025-06-26T14:07:28.108265Z","shell.execute_reply.started":"2025-06-26T14:07:28.102823Z","shell.execute_reply":"2025-06-26T14:07:28.107224Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"\ndef clean_sentence(sentence):\n    \n    \n    sentence = re.sub(r\"(\\w)'(\\w)\", r\"\\1\\2\", sentence)\n    \n    sentence = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", sentence)\n\n    return sentence\n\ndef clean_aspect(aspect):\n    \n    aspect = re.sub(r\"(\\w)'(\\w)\", r\"\\1\\2\", aspect)\n\n    aspect = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", aspect)\n    \n    return aspect.strip()\n\ndef preprocess_file(input_path, output_path):\n\n    def process_aspect(sentence_orig, aspect):\n        # Extract and validate aspect info\n        polarity = aspect.get(\"polarity\", \"\").strip()\n        try:\n            idx_from = int(aspect.get(\"from\", 0))\n            idx_to = int(aspect.get(\"to\", 0))\n        except ValueError:\n            idx_from, idx_to = 0, 0\n\n        # Clean and tokenize sentence parts\n        aspect_substr = sentence_orig[idx_from:idx_to]\n        aspect_clean = clean_aspect(aspect_substr)\n        before = clean_sentence(sentence_orig[:idx_from])\n        after = clean_sentence(sentence_orig[idx_to:])\n        \n        tokens_before = before.split()\n        tokens_after = after.split()\n        aspect_tokens = aspect_clean.split()\n        \n        # Combine tokens and get aspect index\n        tokens = tokens_before + aspect_tokens + tokens_after\n        token_idx = len(tokens_before)\n\n        if not aspect_clean:\n            warnings.warn(f\"Extracted aspect term is empty for sentence: {sentence_orig}\")\n            token_idx = 0\n\n        return {\n            \"tokens\": tokens,\n            \"polarity\": polarity,\n            \"aspect_term\": aspect_clean,\n            \"index\": token_idx\n        }\n\n    # Main processing\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    \n    processed = []\n    for item in data:\n        sentence_orig = item.get(\"sentence\", \"\")\n        for aspect in item.get(\"aspect_terms\", []):\n            instance = process_aspect(sentence_orig, aspect)\n            processed.append(instance)\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(processed, f, indent=2)\n    print(f\"Saved preprocessed data to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.109822Z","iopub.execute_input":"2025-06-26T14:07:28.110094Z","iopub.status.idle":"2025-06-26T14:07:28.131359Z","shell.execute_reply.started":"2025-06-26T14:07:28.110073Z","shell.execute_reply":"2025-06-26T14:07:28.130424Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class ABSADataset(Dataset):\n    def __init__(self, data_path, vocab=None, max_len=50):\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            self.data = json.load(f)\n        self.max_len = max_len\n        if vocab is None:\n            self.build_vocab()\n        else:\n            self.vocab = vocab\n            with open(\"/kaggle/working/vocab_task2.json\",\"w\") as f:\n              json.dump(self.vocab, f)\n            print(\"saved\")\n        self.label_map = {\"positive\": 0, \"negative\": 1, \"neutral\": 2, \"conflict\": 3}\n    \n    def build_vocab(self):\n        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n        idx = 2\n        for item in self.data:\n            for token in item[\"tokens\"]:\n                token_lower = token.lower()\n                if token_lower not in self.vocab:\n                    self.vocab[token_lower] = idx\n                    idx += 1\n    \n    def encode(self, tokens):\n        def get_token_indices(tokens):\n            return [self.vocab.get(token.lower(), self.vocab[\"<UNK>\"]) for token in tokens]\n        \n        def pad_indices(indices):\n            if len(indices) < self.max_len:\n                indices += [self.vocab[\"<PAD>\"]] * (self.max_len - len(indices))\n            else:\n                indices = indices[:self.max_len]\n            return indices\n            \n        indices = get_token_indices(tokens)\n        return pad_indices(indices)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n\n        def prepare_input_tensors(tokens, aspect_index):\n            encoded = self.encode(tokens)\n            aspect_index = min(aspect_index, self.max_len - 1)\n            return {\n                \"input_ids\": torch.tensor(encoded, dtype=torch.long),\n                \"aspect_index\": torch.tensor(aspect_index, dtype=torch.long)\n            }\n\n        def get_label(polarity):\n            return torch.tensor(self.label_map.get(polarity.lower(), 2), dtype=torch.long)\n\n        item = self.data[idx]\n        tensors = prepare_input_tensors(item[\"tokens\"], item[\"index\"])\n        tensors[\"label\"] = get_label(item[\"polarity\"])\n        return tensors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.132435Z","iopub.execute_input":"2025-06-26T14:07:28.132734Z","iopub.status.idle":"2025-06-26T14:07:28.157210Z","shell.execute_reply.started":"2025-06-26T14:07:28.132715Z","shell.execute_reply":"2025-06-26T14:07:28.156596Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class ABSA_RNN_AspectAttention(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=128, output_dim=4, dropout_rate=0.5, pretrained_embeddings=None):\n        super(ABSA_RNN_AspectAttention, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        if pretrained_embeddings is not None:\n            self.embedding.weight.data.copy_(pretrained_embeddings)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, \n                            batch_first=True, dropout=dropout_rate)\n        \n        \n        \n        self.attention_fc = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        self.attention_aspect = nn.Linear(embedding_dim, hidden_dim)\n        \n        self.attention_v = nn.Linear(hidden_dim, 1)\n        \n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n    \n    def forward(self, input_ids, aspect_index):\n\n        def process_lstm_output():\n            embeds = self.embedding(input_ids)               \n            embeds = self.dropout(embeds)\n            lstm_out, _ = self.lstm(embeds)                     \n            lstm_out = self.dropout(lstm_out)\n            \n            batch_size = input_ids.size(0)\n            aspect_emb = embeds[torch.arange(batch_size, device=input_ids.device), aspect_index]\n            aspect_emb_exp = aspect_emb.unsqueeze(1).repeat(1, lstm_out.size(1), 1)\n            return lstm_out, aspect_emb_exp\n\n        def compute_attention(lstm_out, aspect_emb_exp):\n            attn_hidden = torch.tanh(self.attention_fc(lstm_out) + self.attention_aspect(aspect_emb_exp))\n            attn_scores = self.attention_v(attn_hidden)\n            attn_weights = torch.softmax(attn_scores, dim=1)\n            context = torch.sum(attn_weights * lstm_out, dim=1)\n            return context\n\n        lstm_out, aspect_emb_exp = process_lstm_output()\n        context = compute_attention(lstm_out, aspect_emb_exp)\n        logits = self.fc(context)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.158706Z","iopub.execute_input":"2025-06-26T14:07:28.158990Z","iopub.status.idle":"2025-06-26T14:07:28.179550Z","shell.execute_reply.started":"2025-06-26T14:07:28.158964Z","shell.execute_reply":"2025-06-26T14:07:28.178893Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch in train_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        labels = batch[\"label\"].to(device)\n        aspect_index = batch[\"aspect_index\"].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, aspect_index)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        optimizer.step()\n        \n        epoch_loss += loss.item() * input_ids.size(0)\n        preds = outputs.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    \n    return epoch_loss / total, correct / total\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            labels = batch[\"label\"].to(device)\n            aspect_index = batch[\"aspect_index\"].to(device)\n            \n            outputs = model(input_ids, aspect_index)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * input_ids.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            \n    return val_loss / total, correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.286575Z","iopub.execute_input":"2025-06-26T14:07:28.286950Z","iopub.status.idle":"2025-06-26T14:07:28.295708Z","shell.execute_reply.started":"2025-06-26T14:07:28.286923Z","shell.execute_reply":"2025-06-26T14:07:28.294753Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device=\"cpu\"):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n    \n    best_val_acc = 0.0\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f} acc {train_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n        \n        scheduler.step()\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), \"best_custom_model.pt\")\n            print(\"Best custom model saved.\")\n    \n    plt.figure()\n    plt.plot(range(1, epochs+1), train_losses, label=\"Train Loss\")\n    plt.plot(range(1, epochs+1), val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(\"custom_model_loss.png\")\n    plt.close()\n    \n    return best_val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.297005Z","iopub.execute_input":"2025-06-26T14:07:28.297247Z","iopub.status.idle":"2025-06-26T14:07:28.312087Z","shell.execute_reply.started":"2025-06-26T14:07:28.297228Z","shell.execute_reply":"2025-06-26T14:07:28.311500Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def test_model(model, test_dataset, device=\"cpu\"):\n\n    def prepare_evaluation():\n        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n        model.eval()\n        return test_loader\n\n    def compute_accuracy(test_loader):\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in test_loader:\n                input_ids = batch[\"input_ids\"].to(device)\n                labels = batch[\"label\"].to(device)\n                aspect_index = batch[\"aspect_index\"].to(device)\n                outputs = model(input_ids, aspect_index)\n                preds = outputs.argmax(dim=1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n        return correct / total\n\n    test_loader = prepare_evaluation()\n    accuracy = compute_accuracy(test_loader)\n    print(f\"Test Accuracy (Custom Model): {accuracy:.4f}\")\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.313231Z","iopub.execute_input":"2025-06-26T14:07:28.313998Z","iopub.status.idle":"2025-06-26T14:07:28.332621Z","shell.execute_reply.started":"2025-06-26T14:07:28.313979Z","shell.execute_reply":"2025-06-26T14:07:28.331761Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"\ndef load_glove_word_vectors(embedding_file, embedding_dim=300):\n    \n    embeddings_index = {}\n    with open(embedding_file, 'r', encoding=\"utf-8\") as f:\n        for line in f:\n            values = line.rstrip().split(' ')\n            word = values[0]\n            try:\n                vector = np.asarray(values[1:], dtype='float32')\n                if vector.shape[0] == embedding_dim:\n                    embeddings_index[word] = vector\n            except:\n                continue\n    return embeddings_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.334228Z","iopub.execute_input":"2025-06-26T14:07:28.334667Z","iopub.status.idle":"2025-06-26T14:07:28.350742Z","shell.execute_reply.started":"2025-06-26T14:07:28.334645Z","shell.execute_reply":"2025-06-26T14:07:28.350078Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def create_embedding_matrix(vocab, embeddings_index, embedding_dim=300):\n    \n    vocab_size = len(vocab)\n    embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim)).astype('float32')\n    for word, i in vocab.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return torch.tensor(embedding_matrix)\n\ndef load_glove_embeddings(embedding_file, vocab, embedding_dim=300):\n    \n    embeddings_index = load_glove_word_vectors(embedding_file, embedding_dim)\n    return create_embedding_matrix(vocab, embeddings_index, embedding_dim)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.351523Z","iopub.execute_input":"2025-06-26T14:07:28.351757Z","iopub.status.idle":"2025-06-26T14:07:28.367740Z","shell.execute_reply.started":"2025-06-26T14:07:28.351712Z","shell.execute_reply":"2025-06-26T14:07:28.367053Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    test = False\n\n    # Determine device: use GPU if available, otherwise CPU.\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n    \n    # Preprocess train and validation files\n    preprocess_file(\"/kaggle/input/aspect-based-sent-analysis/train.json\", \"train_task2.json\")\n    preprocess_file(\"/kaggle/input/aspect-based-sent-analysis/val.json\", \"val_task2.json\")\n    \n    # Create datasets and dataloaders for custom model training\n    train_dataset = ABSADataset(\"train_task2.json\", max_len=50)\n    val_dataset = ABSADataset(\"val_task2.json\", vocab=train_dataset.vocab, max_len=50)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    \n    \n    glove_embedding_file = \"/kaggle/input/aspect-based-sent-analysis/glove.6B.300d.txt\"\n    pretrained_embeddings = load_glove_embeddings(glove_embedding_file, train_dataset.vocab, embedding_dim=300)\n    \n    # Initialize and train custom RNN model with pre-trained GloVe embeddings\n    model_custom = ABSA_RNN_AspectAttention(vocab_size=len(train_dataset.vocab),\n                                         embedding_dim=300,\n                                         hidden_dim=256,\n                                         output_dim=4,\n                                         dropout_rate=0.5,\n                                         pretrained_embeddings=pretrained_embeddings)\n    \n    model_custom.to(device)\n\n    best_val_acc = train_model(model_custom, train_loader, val_loader, epochs=10, lr=1e-3, device=device)\n    print(f\"Best Validation Accuracy (Custom Model): {best_val_acc:.4f}\")\n\n\n    preprocess_file(\"/kaggle/input/aspect-based-sent-analysis/test.json\", \"test_task2.json\")\n    test_dataset = ABSADataset(\"test_task2.json\", vocab=train_dataset.vocab, max_len=50)\n    model_custom_loaded = ABSA_RNN_AspectAttention(vocab_size=len(train_dataset.vocab),\n                                         embedding_dim=300,\n                                         hidden_dim=256,\n                                         output_dim=4,\n                                         dropout_rate=0.5,\n                                         pretrained_embeddings=pretrained_embeddings)\n\n    model_custom_loaded.load_state_dict(torch.load(\"best_custom_model.pt\", map_location=device))\n    model_custom_loaded.to(device)\n    test_accuracy = test_model(model_custom_loaded, test_dataset, device=device)\n    \n    print(\"\\nTesting complete. Final Test Accuracy (Custom Model):\", test_accuracy)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:07:28.368492Z","iopub.execute_input":"2025-06-26T14:07:28.369022Z","iopub.status.idle":"2025-06-26T14:08:10.383689Z","shell.execute_reply.started":"2025-06-26T14:07:28.369003Z","shell.execute_reply":"2025-06-26T14:08:10.382725Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nSaved preprocessed data to train_task2.json\nSaved preprocessed data to val_task2.json\nsaved\nEpoch 1: Train loss 0.9909 acc 0.5981 | Val loss 1.0132 acc 0.5580\nBest custom model saved.\nEpoch 2: Train loss 0.8052 acc 0.6751 | Val loss 0.9349 acc 0.6496\nBest custom model saved.\nEpoch 3: Train loss 0.6880 acc 0.7281 | Val loss 0.8873 acc 0.6469\nEpoch 4: Train loss 0.5938 acc 0.7700 | Val loss 0.9303 acc 0.6442\nEpoch 5: Train loss 0.5498 acc 0.7889 | Val loss 0.9578 acc 0.6523\nBest custom model saved.\nEpoch 6: Train loss 0.4945 acc 0.8072 | Val loss 0.9379 acc 0.6280\nEpoch 7: Train loss 0.4566 acc 0.8173 | Val loss 1.0135 acc 0.6253\nEpoch 8: Train loss 0.4259 acc 0.8322 | Val loss 0.9307 acc 0.6388\nEpoch 9: Train loss 0.3928 acc 0.8423 | Val loss 0.9911 acc 0.6388\nEpoch 10: Train loss 0.3875 acc 0.8436 | Val loss 0.9845 acc 0.6442\nBest Validation Accuracy (Custom Model): 0.6523\nSaved preprocessed data to test_task2.json\nsaved\nTest Accuracy (Custom Model): 0.6250\n\nTesting complete. Final Test Accuracy (Custom Model): 0.625\n","output_type":"stream"}],"execution_count":41}]}